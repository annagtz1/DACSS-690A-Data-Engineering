{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44a0acdf-e5ec-4ac3-8ee9-6eb0ace14118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote prefect_olist_pipeline.py\n",
      "Wrote generate_data.py\n",
      "Wrote upload_to_s3.sh\n",
      "Wrote deployment_example.yaml\n",
      "Wrote README.md\n",
      "Wrote requirements.txt\n",
      "Wrote .gitignore\n",
      "Made upload_to_s3.sh executable\n",
      "\n",
      "All files created in C:\\Users\\analy\\iCloudDrive\\Desktop\\DACSS Materials and Job Hunt\\DACSS 690A Data Engineering\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import stat\n",
    "\n",
    "files = {\n",
    "\"prefect_olist_pipeline.py\": r'''#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Minimal Prefect ETL for final project (meant to be run locally).\n",
    "- Reads a CSV (local path or s3://)\n",
    "- Enriches with exchangerate.host\n",
    "- Writes results locally under pipeline_outputs/ (S3 optional)\n",
    "\"\"\"\n",
    "from pathlib import Path\n",
    "import io\n",
    "import json\n",
    "from typing import Optional, Dict, Any\n",
    "import pandas as pd\n",
    "import requests\n",
    "import boto3\n",
    "from prefect import flow, task, get_run_logger\n",
    "\n",
    "OUTPUT_DIR = Path(\"pipeline_outputs\")\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "RATES_CACHE_FILE = OUTPUT_DIR / \"rates_cache.json\"\n",
    "DEFAULT_ORDERS_CSV = r\"C:\\Users\\analy\\iCloudDrive\\Desktop\\DACSS Materials and Job Hunt\\DACSS 690A Data Engineering\\archive (4)\\olist_orders_dataset.csv\"\n",
    "\n",
    "def load_rates_cache() -> Dict[str, Optional[float]]:\n",
    "    if RATES_CACHE_FILE.exists():\n",
    "        try:\n",
    "            return json.loads(RATES_CACHE_FILE.read_text())\n",
    "        except Exception:\n",
    "            return {}\n",
    "    return {}\n",
    "\n",
    "def save_rates_cache(cache: Dict[str, Optional[float]]):\n",
    "    try:\n",
    "        RATES_CACHE_FILE.write_text(json.dumps(cache))\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "@task(retries=1, retry_delay_seconds=5)\n",
    "def extract_csv(path: str) -> pd.DataFrame:\n",
    "    logger = get_run_logger()\n",
    "    logger.info(f\"Reading CSV: {path}\")\n",
    "    df = pd.read_csv(path)\n",
    "    logger.info(f\"Loaded rows={len(df)} cols={len(df.columns)}\")\n",
    "    return df\n",
    "\n",
    "@task(retries=2, retry_delay_seconds=5)\n",
    "def fetch_rate_for_date(date_str: str) -> Optional[float]:\n",
    "    logger = get_run_logger()\n",
    "    try:\n",
    "        resp = requests.get(f\"https://api.exchangerate.host/{date_str}\", params={\"base\":\"BRL\",\"symbols\":\"USD\"}, timeout=10)\n",
    "        resp.raise_for_status()\n",
    "        rate = resp.json().get(\"rates\", {}).get(\"USD\")\n",
    "        logger.info(f\"Fetched rate for {date_str}: {rate}\")\n",
    "        return float(rate) if rate is not None else None\n",
    "    except Exception:\n",
    "        logger.exception(\"Failed to fetch rate\")\n",
    "        return None\n",
    "\n",
    "@task\n",
    "def enrich_orders(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    logger = get_run_logger()\n",
    "    if \"order_purchase_timestamp\" not in df.columns:\n",
    "        logger.warning(\"No order_purchase_timestamp column; skipping enrichment\")\n",
    "        df[\"brl_to_usd_rate\"] = None\n",
    "        df[\"payment_usd\"] = None\n",
    "        return df\n",
    "    df = df.copy()\n",
    "    df[\"order_purchase_timestamp\"] = pd.to_datetime(df[\"order_purchase_timestamp\"], errors=\"coerce\")\n",
    "    df[\"date_only\"] = df[\"order_purchase_timestamp\"].dt.date.astype(str)\n",
    "    cache = load_rates_cache()\n",
    "    dates = df[\"date_only\"].dropna().unique().tolist()\n",
    "    for d in dates:\n",
    "        if d not in cache:\n",
    "            # call the task function directly; Prefect will run tasks when the flow executes\n",
    "            cache[d] = fetch_rate_for_date(d)\n",
    "    # Coerce any Prefect task-returned objects to simple floats where possible\n",
    "    for k, v in list(cache.items()):\n",
    "        if isinstance(v, float) or v is None:\n",
    "            continue\n",
    "        try:\n",
    "            cache[k] = float(v)\n",
    "        except Exception:\n",
    "            cache[k] = None\n",
    "    save_rates_cache(cache)\n",
    "    df[\"brl_to_usd_rate\"] = df[\"date_only\"].map(cache)\n",
    "    amt_col = \"price\" if \"price\" in df.columns else \"payment_value\" if \"payment_value\" in df.columns else None\n",
    "    if amt_col:\n",
    "        df[\"payment_usd\"] = df[amt_col] * df[\"brl_to_usd_rate\"]\n",
    "    else:\n",
    "        df[\"payment_usd\"] = None\n",
    "    df = df.drop(columns=[\"date_only\"])\n",
    "    missing = int(df[\"brl_to_usd_rate\"].isna().sum()) if \"brl_to_usd_rate\" in df.columns else 0\n",
    "    if missing > 0:\n",
    "        logger.warning(f\"{missing} rows missing conversion rate\")\n",
    "    return df\n",
    "\n",
    "@task(retries=2, retry_delay_seconds=5)\n",
    "def upload_df_to_s3(df: pd.DataFrame, bucket: str, key: str) -> str:\n",
    "    logger = get_run_logger()\n",
    "    logger.info(f\"Uploading to s3://{bucket}/{key}\")\n",
    "    csv_buffer = io.StringIO()\n",
    "    df.to_csv(csv_buffer, index=False)\n",
    "    s3 = boto3.client(\"s3\")\n",
    "    s3.put_object(Bucket=bucket, Key=key, Body=csv_buffer.getvalue().encode(\"utf-8\"))\n",
    "    url = f\"s3://{bucket}/{key}\"\n",
    "    logger.info(f\"Uploaded to {url}\")\n",
    "    return url\n",
    "\n",
    "@flow(name=\"olist_csv_api_pipeline\")\n",
    "def data_processing_flow(csv_orders: str = DEFAULT_ORDERS_CSV, s3_bucket: str = \"\") -> dict:\n",
    "    logger = get_run_logger()\n",
    "    logger.info(\"Starting minimal ETL\")\n",
    "    orders = extract_csv(csv_orders)\n",
    "    enriched = enrich_orders(orders)\n",
    "    try:\n",
    "        enriched[\"order_purchase_timestamp\"] = pd.to_datetime(enriched[\"order_purchase_timestamp\"], errors=\"coerce\")\n",
    "        enriched[\"month\"] = enriched[\"order_purchase_timestamp\"].dt.to_period(\"M\")\n",
    "        monthly = enriched.groupby(\"month\")[\"payment_usd\"].sum().reset_index().rename(columns={\"payment_usd\":\"monthly_sales_usd\"})\n",
    "    except Exception:\n",
    "        monthly = pd.DataFrame()\n",
    "    result = {\"monthly\": None, \"enriched\": None}\n",
    "    if s3_bucket:\n",
    "        result[\"monthly\"] = upload_df_to_s3(monthly, s3_bucket, \"processed/monthly_sales_usd.csv\")\n",
    "        result[\"enriched\"] = upload_df_to_s3(enriched, s3_bucket, \"processed/enriched_orders.csv\")\n",
    "    else:\n",
    "        monthly.to_csv(OUTPUT_DIR/\"monthly_sales_usd.csv\", index=False)\n",
    "        enriched.to_csv(OUTPUT_DIR/\"enriched_orders.csv\", index=False)\n",
    "        result[\"monthly\"] = str(OUTPUT_DIR/\"monthly_sales_usd.csv\")\n",
    "        result[\"enriched\"] = str(OUTPUT_DIR/\"enriched_orders.csv\")\n",
    "    logger.info(\"ETL done\")\n",
    "    return result\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data_processing_flow()\n",
    "''',\n",
    "\n",
    "\"generate_data.py\": r'''\"\"\"\n",
    "generate_data.py\n",
    "Creates data/orders_large.csv by repeating a tiny sample until the file size >= 100MB.\n",
    "Run: python generate_data.py\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "OUT = Path(\"data\")\n",
    "OUT.mkdir(exist_ok=True)\n",
    "sample_path = OUT / \"orders_sample.csv\"\n",
    "large_path = OUT / \"orders_large.csv\"\n",
    "\n",
    "if not sample_path.exists():\n",
    "    df = pd.DataFrame({\n",
    "        \"order_id\": [f\"o{i}\" for i in range(1, 11)],\n",
    "        \"order_purchase_timestamp\": [\"2020-01-01 10:00:00\"]*10,\n",
    "        \"order_approved_at\": [\"2020-01-01 10:05:00\"]*10,\n",
    "        \"order_delivered_carrier_date\": [\"2020-01-05 10:00:00\"]*10,\n",
    "        \"price\": [10.0, 20.0, 30.0, 5.0, 7.5, 12.0, 6.0, 8.0, 15.0, 3.0]\n",
    "    })\n",
    "    df.to_csv(sample_path, index=False)\n",
    "else:\n",
    "    df = pd.read_csv(sample_path)\n",
    "\n",
    "target_bytes = 100 * 1024 * 1024\n",
    "chunks = []\n",
    "current_bytes = 0\n",
    "rep = 0\n",
    "while current_bytes < target_bytes:\n",
    "    chunks.append(df.assign(rep=rep))\n",
    "    rep += 1\n",
    "    current_bytes = sum(len(chunk.to_csv(index=False).encode(\"utf-8\")) for chunk in chunks)\n",
    "\n",
    "big_df = pd.concat(chunks, ignore_index=True)\n",
    "big_df.to_csv(large_path, index=False)\n",
    "print(f\"Wrote {large_path} size: {large_path.stat().st_size / (1024*1024):.2f} MB\")\n",
    "''',\n",
    "\n",
    "\"upload_to_s3.sh\": r'''#!/usr/bin/env bash\n",
    "# usage: ./upload_to_s3.sh <bucket-name>\n",
    "BUCKET=\"$1\"\n",
    "if [ -z \"$BUCKET\" ]; then\n",
    "  echo \"Usage: $0 <bucket-name>\"\n",
    "  exit 1\n",
    "fi\n",
    "aws s3 cp data/orders_large.csv s3://$BUCKET/raw/orders_large.csv\n",
    "echo \"Uploaded to s3://$BUCKET/raw/orders_large.csv\"\n",
    "''',\n",
    "\n",
    "\"deployment_example.yaml\": r'''apiVersion: prefect.io/v2\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: olist-test\n",
    "  flow_name: olist_csv_api_pipeline\n",
    "spec:\n",
    "  flow_location: \"./prefect_olist_pipeline.py\"\n",
    "  entrypoint: \"data_processing_flow:data_processing_flow\"\n",
    "  schedule:\n",
    "    cron: \"*/5 * * * *\"   # test schedule every 5 minutes; change to \"0 2 * * *\" for daily\n",
    "  parameters:\n",
    "    csv_orders: \"s3://<your-bucket>/raw/orders_large.csv\"\n",
    "    s3_bucket: \"<your-bucket>\"\n",
    "  infra:\n",
    "    type: \"process\"\n",
    "''',\n",
    "\n",
    "\"README.md\": r'''#E-commerce ETL — Local Prefect Run\n",
    "\n",
    "This repo contains a minimal Prefect ETL pipeline and supporting files for the course final project.\n",
    "\n",
    "What I ran (local, reproducible)\n",
    "- I created the pipeline files (prefect_olist_pipeline.py and helpers) using the project setup cell.\n",
    "- I installed the required packages in the notebook environment and restarted the kernel.\n",
    "- I executed a quick sample run and then a full run of the Prefect flow (data_processing_flow) locally.\n",
    "- The pipeline read the Olist orders CSV (local path), attempted to enrich rows with historical BRL→USD exchange rates, and saved outputs to `pipeline_outputs/`.\n",
    "\n",
    "Important reproducibility note\n",
    "- The exchange-rate API returned nulls for the enrichment during my run (see `pipeline_outputs/rates_cache.json`), so for reproducibility I produced monthly totals using a fixed conversion rate (1 BRL = 0.25 USD). Files to inspect:\n",
    "  - `pipeline_outputs/enriched_orders.csv` — enriched dataset from the flow (brl_to_usd_rate and payment_usd columns may be null in this run)\n",
    "  - `pipeline_outputs/monthly_sales_brl_minimal.csv` — monthly totals in BRL\n",
    "  - `pipeline_outputs/monthly_sales_usd_fixedrate_minimal.csv` — monthly USD totals computed using the fixed rate (0.25 BRL→USD)\n",
    "\n",
    "Notes and how to reproduce locally\n",
    "1. Place the Olist CSV in a local path and confirm its location.\n",
    "2. Install requirements:\n",
    "   pip install -r requirements.txt\n",
    "3. Run the pipeline from a notebook (sample first, then full run) or:\n",
    "   python prefect_olist_pipeline.py\n",
    "4. Check outputs in `pipeline_outputs/`.\n",
    "\n",
    "Data policy\n",
    "- The large CSV is intentionally excluded from the repository (see `.gitignore`) to avoid committing big files.\n",
    "''',\n",
    "\n",
    "\"requirements.txt\": r'''prefect>=2.10.0\n",
    "pandas\n",
    "requests\n",
    "boto3\n",
    "s3fs\n",
    "''',\n",
    "\n",
    "\".gitignore\": r'''__pycache__/\n",
    "pipeline_outputs/\n",
    "data/orders_large.csv\n",
    "venv/\n",
    ".env\n",
    ".ipynb_checkpoints/\n",
    "'''\n",
    "}\n",
    "\n",
    "# write files\n",
    "for fname, content in files.items():\n",
    "    Path(fname).write_text(content)\n",
    "    print(\"Wrote\", fname)\n",
    "\n",
    "# make upload_to_s3.sh executable if present\n",
    "sh = Path(\"upload_to_s3.sh\")\n",
    "if sh.exists():\n",
    "    mode = sh.stat().st_mode\n",
    "    sh.chmod(mode | stat.S_IXUSR | stat.S_IXGRP | stat.S_IXOTH)\n",
    "    print(\"Made upload_to_s3.sh executable\")\n",
    "\n",
    "print(\"\\nAll files created in\", Path.cwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3311ad-6e69-48f7-868a-c7d622c87d4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
